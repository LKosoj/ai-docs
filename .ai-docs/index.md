# Документация проекта

## Структура и артефакты

После выполнения `ai_docs` в проекте появляются следующие артефакты:

- `.ai-docs/` — основная директория сгенерированной документации:
  - `overview.md` — общее описание архитектуры проекта.
  - `index.md` — оглавление с пагинацией (до 100 элементов на странице).
  - `modules/` — документация по модулям (автоматически разбита по директориям).
  - `configs/` — описание конфигурационных файлов.
  - `changes.md` — отчёт об изменениях (новые, изменённые, удалённые файлы).
  - `_index.json` — навигационный индекс с приоритетами и маршрутами.
- `.ai_docs_cache/` — кэш LLM-запросов и хешей файлов:
  - `llm_cache.json` — закэшированные ответы модели.
  - `index.json` — метаданные файлов (хеши, размеры, типы).
- `README.md` — краткое описание проекта (генерируется при флаге `--readme`).
- `mkdocs.yml` — конфигурация сайта документации.
- `ai_docs_site/` — собранный статический сайт (результат `mkdocs build`).

> **Примечание**: Все перечисленные директории и файлы включены в `.gitignore` по умолчанию. Для публикации документации используйте CI/CD.

---

## Конфигурация

### `.ai-docs.yaml` (опционально)

Файл позволяет кастомизировать поведение сканера. Создаётся автоматически при первом запуске.

```yaml
code_extensions:
  .py: Python
  .go: Go
  .ts: TypeScript
doc_extensions:
  - .md
  - .rst
config_extensions:
  - .yaml
  - .json
  - .tf
exclude:
  - temp/*
  - *.log
  - .github/*
```

- `code_extensions` — сопоставление расширений с языком программирования.
- `doc_extensions` — файлы, распознаваемые как документация.
- `config_extensions` — конфигурационные файлы, обрабатываемые отдельно.
- `exclude` — пользовательские шаблоны исключения (дополняют `.gitignore`, `.build_ignore` и встроенные правила).

---

## Переменные окружения

| Переменная | Назначение | Значение по умолчанию |
|-----------|------------|------------------------|
| `OPENAI_API_KEY` | Ключ API для доступа к LLM | Обязательно |
| `OPENAI_BASE_URL` | Альтернативный эндпоинт (например, для локальных моделей) | `https://api.openai.com/v1` |
| `OPENAI_MODEL` | Модель для генерации | `gpt-4o-mini` |
| `OPENAI_MAX_TOKENS` | Максимальное число токенов в ответе | `4096` |
| `OPENAI_CONTEXT_TOKENS` | Лимит контекста модели | `32768` |
| `OPENAI_TEMPERATURE` | Уровень креативности генерации | `0.3` |
| `AI_DOCS_THREADS` | Число потоков параллельной обработки | `4` |
| `AI_DOCS_LOCAL_SITE` | Режим локального хостинга (без публикации) | `false` |

---

## CLI-команды

### Основная команда

```bash
ai-docs --source <путь_или_url> [--readme|--mkdocs] [опции]
```

### Ключевые опции

| Опция | Назначение |
|------|-----------|
| `--source` | Путь к локальной директории или URL Git-репозитория |
| `--readme` | Генерировать `README.md` |
| `--mkdocs` | Генерировать `mkdocs.yml` и подготовить сайт |
| `--language ru/en` | Язык документации (`ru` по умолчанию) |
| `--force` | Перезаписать существующие файлы |
| `--regen section1,section2` | Принудительно перегенерировать секции (например, `architecture`, `configs`) |
| `--threads N` | Количество параллельных потоков (переопределяет `AI_DOCS_THREADS`) |
| `--local-site` | Настроить `mkdocs.yml` для локального запуска (`serve`) |
| `--no-cache` | Отключить кэширование LLM-запросов |
| `--cache-dir <путь>` | Указать кастомную директорию кэша |

### Примеры

```bash
# Генерация README на русском
ai-docs --source . --readme --language ru

# Генерация MkDocs-сайта с перегенерацией архитектуры
ai-docs --source https://github.com/user/repo.git --mkdocs --regen architecture --threads 8

# Локальный запуск с отключённым кэшем
ai-docs --source . --mkdocs --local-site --no-cache
```

---

## Интеграция в CI/CD

Пример для GitHub Actions:

```yaml
name: Build Docs
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install ai-docs
        run: pip install ai-docs-gen
      - name: Generate docs
        run: ai-docs --source . --mkdocs --local-site
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          AI_DOCS_THREADS: 4
      - name: Deploy site
        run: mkdocs build -f mkdocs.yml
```

> **Важно**: Убедитесь, что `ai_docs_site/` публикуется через `gh-pages` или другой хостинг.

---

## Принципы работы

1. **Сканирование**:
   - Анализируется структура проекта с учётом `.gitignore`, `.build_ignore`, встроенных исключений.
   - Определяются домены: Kubernetes, Terraform, Docker, CI/CD и др.
   - Файлы классифицируются как код, конфигурации, документация.

2. **Инкрементальная обработка**:
   - Сравнение хешей файлов с предыдущим запуском.
   - Обрабатываются только новые, изменённые или отсутствующие в кэше файлы.
   - Результаты LLM кэшируются по хешу запроса.

3. **Генерация**:
   - Используется `gpt-4o-mini` (или другая указанная модель) для суммаризации.
   - Контекст укладывается в лимит токенов через иерархическое суммирование.
   - Поддерживается многоязычность и форматирование (Mermaid, списки, таблицы).

4. **Вывод**:
   - Создаётся `README.md` и/или `mkdocs.yml`.
   - Генерируется сайт через `mkdocs build`.
   - Обновляется `changes.md` с детализацией изменений.

---

## Расширение и кастомизация

### Добавление поддержки новых технологий

Обновите `classify_type` и `detect_domains` в `utils/file_types.py`, добавив:
- Расширения файлов.
- Имена ключевых файлов (например, `Chart.yaml` → Helm).
- Пути (например, `terraform/` → Terraform).

### Кастомизация навигации

Измените `build_mkdocs_yaml` или используйте `_index.json` для ручного управления приоритетами и порядком разделов.

### Поддержка локальных LLM

Укажите `OPENAI_BASE_URL` и `OPENAI_API_KEY` (может быть любым токеном, если не требуется аутентификация):

```bash
OPENAI_BASE_URL=http://localhost:1234/v1 \
OPENAI_API_KEY=sk-no-key \
ai-docs --source . --mkdocs
```

---

## Ограничения

- Максимальный размер файла: 200 КБ.
- Бинарные файлы не обрабатываются.
- Тестовые файлы (по шаблонам `test_`, `*_test.*`, `/tests/`) исключаются из модульной документации.
- При первом запуске без кэша обработка может занять несколько минут.

---

## Отладка

- Проверьте `.ai_docs_cache/index.json` — содержит актуальное состояние файлов.
- Убедитесь, что `OPENAI_API_KEY` установлен.
- При ошибках LLM проверьте `errors` в логах или возвращаемых значениях.
- Для диагностики используйте `--no-cache` и `--threads 1`.
