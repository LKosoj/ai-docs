# Документация проекта

## Обзор

`ai_docs` — CLI-инструмент для автоматической генерации технической документации по исходному коду и конфигурационным файлам. Поддерживает анализ локальных директорий, локальных и удалённых Git-репозиториев. Основные выходные артефакты:

- `README.md` — краткое описание проекта.
- `ai_docs_site/` — полноценный сайт документации на основе MkDocs.

Инструмент использует LLM (через OpenAI API или совместимые эндпоинты), поддерживает многопоточную обработку, кэширование и инкрементальную генерацию. Интегрируется в CI/CD и оптимизирован для повторных запусков.

---

## Структура проекта

После выполнения `ai_docs` создаются следующие артефакты:

```
.
├── .ai_docs_cache/          # Кэш LLM-ответов и метаданных
├── .ai-docs/                # Детальная документация
│   ├── index.md             # Главная страница
│   ├── architecture.md      # Архитектура проекта
│   ├── modules/             # Описания модулей
│   ├── configs/             # Документация по конфигурациям
│   └── _index.json          # Навигационный индекс
├── ai_docs_site/            # Сгенерированный MkDocs-сайт
├── mkdocs.yml               # Конфигурация MkDocs
├── docs/changes.md          # Журнал изменений
└── .ai-docs.yaml            # Опциональная кастомизация (настройки фильтрации, расширений)
```

---

## Поддерживаемые технологии и домены

Инструмент автоматически определяет домены по расширениям и именам файлов:

- **Инфраструктура**: Terraform (`.tf`), Kubernetes (`.yaml`, `k8s/`), Helm, Docker (Dockerfile, `.dockerignore`)
- **CI/CD**: GitHub Actions (`.github/workflows`), GitLab CI (`.gitlab-ci.yml`)
- **Языки**: Python, TypeScript, Go, Java, Rust
- **Конфигурации**: `.env`, `.yaml`, `.json`, `.toml`

Файлы классифицируются как:
- `code` — исходный код
- `config` — конфигурации
- `doc` — документация
- `infra` — инфраструктурные манифесты

---

## Конфигурация

### `.ai-docs.yaml` (опционально)

Позволяет переопределить поведение сканирования:

```yaml
code_extensions:
  - .py
  - .ts
  - .go
config_extensions:
  - .yaml
  - .env
doc_extensions:
  - .md
  - .rst
exclude:
  - "*.log"
  - "temp/"
  - "node_modules/"
```

Если файл отсутствует, используется конфиг по умолчанию с поддержкой типичных расширений и шаблонов исключения.

---

## Фильтрация файлов

### Включение
- Всегда включаются: `Dockerfile`, `terraform.tf`, `package.json`, `requirements.txt`, `*.lock`
- Через `--include`: glob-шаблоны (например, `--include "*.py"`)

### Исключение
- По умолчанию: `.git`, `__pycache__`, `.venv`, `node_modules`, `*.pyc`, `*.exe`
- Через `.gitignore` и `.build_ignore`
- Через `--exclude` или секцию `exclude` в `.ai-docs.yaml`

### Ограничения
- `--max-size` — максимальный размер файла в байтах (по умолчанию `200000`, ~200 КБ)
- Бинарные файлы (определяются по наличию нулевых байтов) пропускаются

---

## Работа с источниками

Поддерживаются:
- Локальные пути: `ai_docs --source ./my-project`
- Git-репозитории по URL: `ai_docs --source https://github.com/user/repo.git`

При использовании URL:
- Репозиторий клонируется во временный каталог
- После завершения — временные файлы удаляются

---

## Настройка LLM

### Переменные окружения

```bash
OPENAI_API_KEY=sk-...                # Обязательно
OPENAI_BASE_URL=https://api.openai.com/v1  # Опционально (для кастомных эндпоинтов)
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.2
OPENAI_MAX_TOKENS=1200
OPENAI_CONTEXT_TOKENS=8192
```

Можно использовать `.env`-файл для загрузки переменных.

### Клиент LLM

- `LLMClient` — отправляет запросы к OpenAI-совместимому API
- Поддерживает кэширование по SHA256 от payload
- Потокобезопасен (использует `threading.Lock`)
- Таймауты: 120 сек (подключение), 480 сек (ответ)

---

## Генерация документации

### Режимы работы

| Флаг | Результат |
|------|---------|
| Без флагов | Генерирует `README.md` и `mkdocs.yml` |
| `--readme` | Только `README.md` |
| `--mkdocs` | Только `mkdocs.yml` и структуру `.ai-docs/` |

### Язык

- `--language ru` (по умолчанию) или `--language en`

### Кэширование

- Кэш LLM-ответов: `.ai_docs_cache/llm_cache.json`
- Кэш метаданных файлов: `.ai_docs_cache/index.json`
- Отключение: `--no-cache`

---

## Многопоточность

- Количество потоков: `--threads N` (по умолчанию зависит от CPU)
- Через переменную: `AI_DOCS_THREADS=4`
- Используется `ThreadPoolExecutor` для параллельной обработки файлов

---

## Интеграция с MkDocs

При генерации сайта:
- Создаётся `mkdocs.yml` с динамической навигацией
- Поддержка вложенных разделов: `Модули`, `Конфиги`, `Архитектура`
- При `--local-site` или `AI_DOCS_LOCAL_SITE=1`:
  - `site_url: ""`
  - `use_directory_urls: false`

Сайт собирается в `ai_docs_site/` и готов к локальному запуску:

```bash
cd ai_docs_site && python -m http.server 8000
```

---

## Журнал изменений

После каждого запуска обновляется `docs/changes.md` с информацией:
- Добавленные/изменённые/удалённые файлы
- Перегенерированные разделы
- Краткое резюме изменений

Формат: Markdown с чёткой структурой для отслеживания эволюции документации.

---

## Запуск и использование

### Примеры команд

```bash
# Полная генерация (README + MkDocs)
ai_docs --source . --language ru

# Только README
ai_docs --source . --readme --language en

# С фильтрацией и кастомным размером
ai_docs --source . --include "*.py" --exclude "tests/" --max-size 500000

# Из удалённого репозитория
ai_docs --source https://github.com/user/project.git --output ./docs

# С локальным хостингом MkDocs
ai_docs --source . --mkdocs --local-site
```

---

## Интеграция в CI/CD

Рекомендуемый сценарий:

```yaml
- name: Generate Docs
  run: |
    pip install ai_docs
    ai_docs --source . --mkdocs --readme --language ru
  env:
    OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
```

При повторных запусках:
- Сохраняется кэш
- Обрабатываются только изменённые файлы
- Обновляется `changes.md`

---

## Требования

- Python 3.9+
- `git` (для работы с URL-репозиториями)
- Зависимости: `tiktoken`, `PyYAML`, `pathspec`, `python-dotenv`

Установка:
```bash
pip install ai_docs
```

---

## Диагностика и отладка

- Ошибки генерации логируются в stdout
- При сбоях LLM — повторные попытки не выполняются, ошибка сохраняется в отчёт
- Для отладки кэша: проверьте `.ai_docs_cache/llm_cache.json` и `index.json`

---

## Ограничения

- Файлы > `--max-size` пропускаются
- Бинарные файлы не анализируются
- LLM-запросы зависят от доступности API и ключа
- Не поддерживает приватные репозитории с аутентификацией (только публичные или с токеном в URL)
