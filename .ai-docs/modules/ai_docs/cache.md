# ai_docs/cache

Модуль предоставляет класс `CacheManager` для управления кэшем файлов и данных, связанных с обработкой текстов и LLM-кэшированием.  
Он хранит индекс файлов и кэш ответов LLM в виде JSON-файлов на диске.  
Класс поддерживает загрузку и сохранение данных, а также вычисление различий между текущим и предыдущим состоянием файлов.

Ключевые структуры данных
Dict — Словарь, представляющий индекс файлов или кэш LLM; структура: {"files": {path: {meta}}, "sections": {...}} или {prompt: response}

class CacheManager
Менеджер кэша для хранения индекса файлов и кэша ответов LLM
Поля
cache_dir — Директория, в которой хранятся файлы кэша
index_path — Путь к файлу index.json с метаданными файлов
llm_cache_path — Путь к файлу llm_cache.json с кэшированными ответами LLM
Методы
__init__(self, cache_dir: Path) — Инициализирует менеджер кэша и создает необходимые директории и файлы
load_index(self) -> Dict — Загружает индекс файлов из index.json; при отсутствии возвращает пустую структуру
save_index(self, data: Dict) -> None — Сохраняет переданный словарь данных в index.json
load_llm_cache(self) -> Dict[str, str] — Загружает кэш LLM из llm_cache.json; при отсутствии возвращает пустой словарь
save_llm_cache(self, data: Dict[str, str]) -> None — Сохраняет переданный словарь в llm_cache.json
diff_files(self, current_files: Dict[str, Dict]) -> Tuple[Dict, Dict, Dict, Dict] — Сравнивает текущие файлы с предыдущим состоянием и возвращает словари добавленных, изменённых, удалённых и неизменённых файлов
