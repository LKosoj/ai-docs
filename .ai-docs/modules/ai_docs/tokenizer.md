# ai_docs/tokenizer

Модуль предоставляет инструменты для подсчёта токенов и разбиения текста на части с учётом ограничений по количеству токенов, используя кодировку, ассоциированную с моделью OpenAI. Основан на библиотеке `tiktoken`. В случае отсутствия модели в реестре используется базовая кодировка `cl100k_base`.

Ключевые структуры данных  
max_tokens — Максимальное количество токенов в одном фрагменте при разбиении текста

get_encoding(model: str)  
Возвращает кодировку токенов для указанной модели, fallback к cl100k_base при отсутствии  
Аргументы  
model — Название модели, для которой запрашивается кодировка  
Возвращает  
Объект кодировки из tiktoken  
Исключения  
Нет исключений — ошибка KeyError перехватывается и обрабатывается

---  
count_tokens(text: str, model: str) -> int  
Подсчитывает количество токенов в тексте для указанной модели  
Аргументы  
text — Входной текст  
model — Название модели для определения кодировки  
Возвращает  
Количество токенов в тексте  
Исключения  
Нет исключений

---  
chunk_text(text: str, model: str, max_tokens: int) -> List[str]  
Разбивает текст на фрагменты, каждый из которых содержит не более max_tokens токенов  
Аргументы  
text — Входной текст для разбиения  
model — Название модели для определения кодировки  
max_tokens — Максимальное количество токенов в одном фрагменте  
Возвращает  
Список строк, каждый элемент — декодированный фрагмент токенов  
Исключения  
Нет исключений
