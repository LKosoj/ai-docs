# ai_docs/tokenizer

Модуль предоставляет инструменты для подсчёта токенов в тексте и его разбиения на части с учётом ограничений по количеству токенов, используя кодировку, ассоциированную с моделью OpenAI. Основан на библиотеке `tiktoken`. В случае отсутствия кодировки для указанной модели используется базовая кодировка `cl100k_base`.

---

def get_encoding(model: str)
Возвращает кодировку токенизатора для указанной модели.
Аргументы
model — Название модели, для которой запрашивается кодировка (например, "gpt-4", "gpt-3.5-turbo").
Возвращает
Объект кодировки из библиотеки `tiktoken`.
Исключения
Ключевое исключение `KeyError` перехватывается внутри функции; при его возникновении возвращается кодировка `cl100k_base`.

---

def count_tokens(text: str, model: str) -> int
Подсчитывает количество токенов в тексте для указанной модели.
Аргументы
text — Входной текст, токены которого необходимо подсчитать.
model — Название модели, используемой для определения кодировки.
Возвращает
Количество токенов в закодированном тексте.

---

def chunk_text(text: str, model: str, max_tokens: int) -> List[str]
Разбивает текст на фрагменты, каждый из которых содержит не более `max_tokens` токенов.
Аргументы
text — Исходный текст для разбиения.
model — Название модели, определяющей кодировку токенизации.
max_tokens — Максимальное количество токенов в одном фрагменте.
Возвращает
Список строк, каждый элемент — декодированный фрагмент текста длиной до `max_tokens` токенов.
