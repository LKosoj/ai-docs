# ai_docs/cache

```markdown
Модуль предоставляет класс `CacheManager` для управления кэшем файлов и данных, связанных с обработкой текстов и LLM-кэшированием. Он хранит индекс файлов и кэш ответов модели в JSON-файлах, обеспечивая загрузку, сохранение и сравнение состояний. Класс поддерживает восстановление после повреждения данных и безопасную запись с проверкой кодировки.

Ключевые структуры данных
Dict[str, Dict] — Словарь, представляющий метаданные файлов, где ключ — путь, значение — словарь с полями, например, hash.
Dict[str, str] — Кэш запросов и ответов LLM, где ключ — хэш запроса, значение — текст ответа.

class CacheManager
Менеджер кэша для хранения индекса файлов и кэша ответов LLM в файловой системе
Поля
cache_dir — Директория, в которой хранятся все кэшированные данные
index_path — Путь к файлу index.json с информацией о проиндексированных файлах
llm_cache_path — Путь к файлу llm_cache.json с кэшем ответов LLM
Методы
__init__(self, cache_dir: Path) — Инициализирует менеджер кэша и создает необходимые директории и файлы
load_index(self) -> Dict — Загружает индекс файлов из index.json, возвращает пустую структуру при отсутствии файла
save_index(self, data: Dict) -> None — Сохраняет индекс файлов в index.json с форматированием и поддержкой UTF-8
load_llm_cache(self) -> Dict[str, str] — Загружает кэш LLM из llm_cache.json, возвращает пустой словарь при отсутствии или ошибке
save_llm_cache(self, data: Dict[str, str]) -> None — Сохраняет кэш LLM в llm_cache.json с резервным копированием при повреждении
diff_files(self, current_files: Dict[str, Dict]) -> Tuple[Dict, Dict, Dict, Dict] — Сравнивает текущие файлы с предыдущим индексом и возвращает добавленные, изменённые, удалённые и неизменённые файлы

---
__init__(self, cache_dir: Path)
Инициализирует менеджер кэша, создает директорию кэша и устанавливает пути к файлам
Аргументы
cache_dir — Директория для хранения кэша

---
load_index(self) -> Dict
Загружает индекс файлов из JSON-файла или возвращает начальную структуру при его отсутствии
Возвращает
Словарь с ключами "files" и "sections", представляющий состояние индекса
Исключения
UnicodeDecodeError — При ошибке декодирования файла из-за некорректной кодировки
JSONDecodeError — При повреждении формата JSON в файле index.json

---
save_index(self, data: Dict) -> None
Сохраняет переданный словарь в файл index.json с отступами и поддержкой UTF-8
Аргументы
data — Данные индекса для сохранения
Исключения
TypeError — Если данные не могут быть сериализованы в JSON
IOError — При ошибке записи файла

---
load_llm_cache(self) -> Dict[str, str]
Загружает кэш LLM из JSON-файла; при ошибке парсинга сохраняет повреждённый файл как .bad и возвращает пустой словарь
Возвращает
Словарь вида {request_hash: response_text}, либо пустой словарь при отсутствии или ошибке
Исключения
UnicodeDecodeError — При ошибке декодирования файла из-за некорректной кодировки
JSONDecodeError — При повреждении формата JSON в файле llm_cache.json

---
save_llm_cache(self, data: Dict[str, str]) -> None
Сохраняет кэш LLM в файл, предварительно делая снимок данных
Аргументы
data — Кэш запросов и ответов для сохранения
Исключения
TypeError — Если данные не могут быть сериализованы в JSON
IOError — При ошибке записи файла

---
diff_files(self, current_files: Dict[str, Dict]) -> Tuple[Dict, Dict, Dict, Dict]
Сравнивает текущее состояние файлов с предыдущим индексом и определяет, какие файлы были добавлены, изменены, удалены или остались без изменений
Аргументы
current_files — Текущий словарь файлов с их метаданными, ключ — путь
Возвращает
Кортеж из четырёх словарей: добавленные, изменённые, удалённые, неизменённые файлы
```
