# ai_docs/llm

```markdown
Модуль предоставляет клиент для взаимодействия с LLM (Large Language Model) через HTTP-запросы, поддерживая кэширование ответов и конфигурацию через переменные окружения. Основной класс `LLMClient` инкапсулирует логику отправки запросов к API, формирования полезной нагрузки и безопасной работы с кэшем. Функция `from_env` упрощает создание клиента на основе переменных окружения.

LLMClient
Клиент для отправки запросов к языковой модели с поддержкой кэширования и настройкой параметров генерации.
Поля
api_key — Ключ API для аутентификации в сервисе LLM.
base_url — Базовый URL API, к которому выполняются запросы.
model — Идентификатор модели, используемой для генерации.
temperature — Параметр температуры, контролирующий случайность ответов.
max_tokens — Максимальное количество токенов в сгенерированном ответе.
context_limit — Общий лимит токенов в контексте (вход + выход).
_cache_lock — Потокобезопасная блокировка для защиты доступа к кэшу.

Методы
__init__(api_key: str, base_url: str, model: str, temperature: float = 0.2, max_tokens: int = 1200, context_limit: int = 8192) — Инициализирует клиент с параметрами модели и API.
_cache_key(payload: Dict) -> str — Генерирует хеш-ключ на основе нормализованного JSON-представления полезной нагрузки.
chat(messages: List[Dict[str, str]], cache: Optional[Dict[str, str]] = None) -> str — Отправляет запрос к модели с указанными сообщениями и возвращает сгенерированный ответ.

---
__init__(api_key: str, base_url: str, model: str, temperature: float = 0.2, max_tokens: int = 1200, context_limit: int = 8192)
Инициализирует клиент с параметрами модели и API.
Аргументы
api_key — Ключ API для аутентификации.
base_url — Базовый URL API (например, https://api.openai.com/v1).
model — Имя модели, например gpt-4o-mini.
temperature — Температура генерации (по умолчанию 0.2).
max_tokens — Максимальное количество токенов в ответе (по умолчанию 1200).
context_limit — Лимит токенов в контексте (по умолчанию 8192).

---
_cache_key(payload: Dict) -> str
Генерирует хеш-ключ на основе нормализованного JSON-представления полезной нагрузки.
Аргументы
payload — Словарь с данными запроса, который будет сериализован и захеширован.
Возвращает
SHA256-хеш строки JSON-представления payload с отсортированными ключами.

---
chat(messages: List[Dict[str, str]], cache: Optional[Dict[str, str]] = None) -> str
Отправляет запрос к модели с указанными сообщениями и возвращает сгенерированный ответ.
Аргументы
messages — Список сообщений в формате {"role": "...", "content": "..."}.
cache — Опциональный словарь для кэширования ответов по ключу запроса.
Возвращает
Текст сгенерированного ответа от модели.
Исключения
requests.RequestException — При ошибке HTTP-запроса.
KeyError — Если в ответе отсутствуют ожидаемые поля (например, content).
RuntimeError — Если ответ не содержит данных (например, пустой choices).

---
from_env() -> LLMClient
Создаёт экземпляр LLMClient на основе переменных окружения.
Возвращает
Настроенный экземпляр LLMClient.
Исключения
RuntimeError — Если переменная окружения OPENAI_API_KEY не задана.
```
