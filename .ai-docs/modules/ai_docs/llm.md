# ai_docs/llm

```markdown
Клиент для взаимодействия с моделью LLM через API, поддерживающий кэширование запросов и настройку параметров генерации. Работает с любым совместимым с OpenAI API сервером, позволяет отправлять чат-запросы и управлять контекстом. Кэширование осуществляется по хешу тела запроса, потокобезопасно. Конфигурация может быть загружена из переменных окружения через функцию `from_env`.

Ключевые структуры данных
LLMClient — Клиент для отправки запросов к LLM с поддержкой кэширования и настройкой параметров генерации

class LLMClient
Клиент для взаимодействия с LLM через API с поддержкой кэширования и управления контекстом
Поля
api_key — Ключ аутентификации для доступа к API
base_url — Базовый URL API (с опциональным суффиксом /v1)
model — Имя модели для генерации (например, gpt-4o-mini)
temperature — Параметр температуры для разнообразия ответов
max_tokens — Максимальное количество токенов в ответе
context_limit — Общий лимит токенов в контексте

Методы
LLMClient.__init__(self, api_key: str, base_url: str, model: str, temperature: float = 0.2, max_tokens: int = 1200, context_limit: int = 8192) — Инициализирует клиент для работы с LLM API
---
LLMClient._cache_key(self, payload: dict) — Генерирует SHA256-хеш JSON-представления payload для использования в кэшировании
Аргументы
payload — Словарь с данными запроса
Возвращает
Хешированный ключ в виде строки
---
LLMClient.chat(self, messages: list[dict], cache: dict | None = None) — Отправляет запрос к чат-модели и возвращает сгенерированный ответ
Аргументы
messages — Список сообщений диалога в формате {"role": "...", "content": "..."}
cache — Опциональный словарь для кэширования ответов (ключ — хеш запроса, значение — ответ)
Возвращает
Текст сгенерированного ответа от модели
Исключения
RuntimeError — Если ответ API не содержит ожидаемого поля с контентом
---
from_env() — Создаёт экземпляр LLMClient на основе переменных окружения
Возвращает
Настроенный экземпляр LLMClient
Исключения
RuntimeError — Если не задана переменная окружения OPENAI_API_KEY
```
