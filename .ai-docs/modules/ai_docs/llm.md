# ai_docs/llm

Модуль предоставляет клиент для взаимодействия с LLM (Large Language Model) через HTTP API, поддерживая кэширование ответов и конфигурацию через переменные окружения. Основной класс `LLMClient` инкапсулирует логику отправки запросов к модели, включая управление заголовками, сериализацию и обработку ответов. Функция `from_env` упрощает создание клиента на основе переменных окружения.

Ключевые структуры данных  
LLMClient — Клиент для отправки запросов к языковой модели с поддержкой кэширования и настройкой параметров генерации.

---
class LLMClient  
Клиент для работы с LLM API, поддерживающий отправку чат-запросов, кэширование и настройку параметров генерации.  
Поля  
api_key — Ключ аутентификации для доступа к API.  
base_url — Базовый URL API (например, https://api.openai.com/v1).  
model — Идентификатор модели, используемой для генерации (например, gpt-4o-mini).  
temperature — Параметр температуры для контролирования случайности генерации.  
max_tokens — Максимальное количество токенов в ответе.  
context_limit — Общий лимит токенов в контексте (используется для внутренних проверок).  
Методы  
LLMClient.__init__(self, api_key: str, base_url: str, model: str, temperature: float = 0.2, max_tokens: int = 1200, context_limit: int = 8192) — Инициализирует клиент для работы с LLM API.  
LLMClient._cache_key(self, payload: Dict) -> str — Генерирует хеш-ключ для кэширования на основе нормализованного JSON-представления полезной нагрузки.  
LLMClient.chat(self, messages: List[Dict[str, str]], cache: Optional[Dict[str, str]] = None) -> str — Отправляет запрос к API чата и возвращает сгенерированный ответ модели.

---
from_env() -> LLMClient  
Создаёт и возвращает экземпляр LLMClient, используя параметры из переменных окружения.  
Возвращает  
Настроенный экземпляр LLMClient.  
Исключения  
RuntimeError — Если не задана переменная окружения OPENAI_API_KEY.
