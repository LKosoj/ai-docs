# Архитектура

```mermaid
graph TD
    A[Источник: локальный путь / Git-URL] --> B[Модуль сканирования файлов]
    B --> C{Фильтрация}
    C --> D[Включение: код, конфиги, документация]
    C --> E[Исключение: .git, __pycache__, бинарники]
    C --> F[Ограничение по размеру: --max-size]
    B --> G[Определение типов и доменов: Python, Terraform, Docker и др.]
    G --> H[ScanResult: список файлов с метаданными]
    H --> I[Кэширование состояния: CacheManager]
    I --> J{Изменения?}
    J -->|Да| K[Обработка через LLMClient]
    J -->|Нет| L[Восстановление из кэша]
    K --> M[Суммаризация: summarize_file]
    M --> N[Краткое или детальное описание (Doxygen-стиль)]
    N --> O[Нормализация формата: _normalize_module_summary]
    O --> P[Запись в .ai-docs/modules/]
    P --> Q[Генерация _index.json]
    Q --> R[build_mkdocs_yaml]
    R --> S[mkdocs.yml]
    S --> T[MkDocs-сайт: ai_docs_site/]
    M --> U[Генерация README.md]
    U --> V[format_changes_md: docs/changes.md]
    W[LLMClient] -->|Кэширование| X[.ai_docs_cache/llm_cache.json]
    I -->|Хранение хэшей| Y[.ai_docs_cache/index.json]
    Z[.ai-docs.yaml] --> B
    Z --> R
    AA[Переменные окружения] --> W
    AB[main.py] --> B
    AB --> K
    AB --> R
    AB --> U
```

# Архитектура

Архитектура `ai_docs` построена вокруг модульного, потокового анализа исходных файлов с последующей генерацией документации через LLM. Система оптимизирована для повторных запусков, поддерживает инкрементальное обновление и интеграцию в CI/CD.

## Основные компоненты

### 1. **Сканирование файлов (`scanner.py`)**
- Рекурсивно обходит директорию или клонирует Git-репозиторий.
- Применяет фильтры:
  - Включает файлы по расширениям (`CODE_EXTENSIONS`, `CONFIG_EXTENSIONS`) и ключевым именам (`Dockerfile`, `terraform.tf`).
  - Исключает по шаблонам: `.git`, `__pycache__`, `node_modules`, `.venv`, а также через `.gitignore` и `.build_ignore`.
  - Ограничивает размер файлов (`--max-size`, по умолчанию 200 КБ).
- Определяет тип файла и домены (Kubernetes, CI/CD и др.) через `classify_type` и `detect_domains`.
- Результат — `ScanResult` с метаданными, используемый на всех последующих этапах.

### 2. **Кэширование (`cache.py`)**
- `CacheManager` управляет двумя JSON-файлами:
  - `.ai_docs_cache/index.json` — хранит хэши файлов (SHA-256) и метаданные.
  - `.ai_docs_cache/llm_cache.json` — кэширует ответы LLM по хешу запроса.
- Метод `diff_files` определяет:
  - `added`, `modified` — требуют перегенерации.
  - `deleted` — удаляются из `.ai-docs`.
  - `unchanged` — восстанавливаются из кэша.
- Обеспечивает повторяемость и экономию ресурсов при частичных изменениях.

### 3. **LLM-взаимодействие (`llm.py`)**
- `LLMClient` отправляет запросы к OpenAI-совместимому API.
- Поддерживает:
  - Кастомный `base_url` (для локальных моделей).
  - Кэширование через передаваемый словарь.
  - Потокобезопасность при многопоточной обработке.
- Инициализация через `from_env` с переменными:
  - `OPENAI_API_KEY` — обязательна.
  - `OPENAI_MODEL`, `OPENAI_MAX_TOKENS`, `OPENAI_CONTEXT_TOKENS`.
- Таймауты: 120 сек (connect), 480 сек (read).

### 4. **Суммаризация (`summarizer.py`)**
- `summarize_file` генерирует описание на основе:
  - Содержимого файла.
  - Типа (`code`, `config`, `infra`).
  - Доменов (Terraform, Docker и др.).
  - Режима: краткий (`SUMMARY_PROMPT`) или детальный (`MODULE_SUMMARY_PROMPT`).
- При детальном режиме применяется постобработка:
  - `_needs_doxygen_fix` проверяет формат.
  - `_normalize_module_summary` перезапрашивает у LLM при необходимости.
- Результат сохраняется в `.ai-docs/modules/<rel_path>.md`.

### 5. **Генерация документации (`main.py`)**
- Основной поток:
  1. Сканирование источника.
  2. Сравнение с кэшем.
  3. Параллельная обработка изменённых файлов (`ThreadPoolExecutor`, `--threads`).
  4. Генерация разделов: архитектура, запуск, зависимости.
  5. Построение навигации (`_index.json`).
  6. Формирование `README.md` и/или `mkdocs.yml`.
- Поддерживает:
  - Язык: `--language ru/en`.
  - Режимы: `--readme`, `--mkdocs`, оба по умолчанию.

### 6. **Генерация MkDocs (`mkdocs_builder.py`)**
- `build_mkdocs_yaml` формирует конфиг:
  - Динамическая навигация через `_tree_to_nav`.
  - Поддержка вложенных модулей.
  - Специальные разделы: "Конфиги", "Архитектура".
  - Режим `--local-site`: `site_url: /`, `use_directory_urls: false`.
- `write_docs_files` копирует `.md` в `ai_docs_site/`.

### 7. **Вспомогательные модули**
- `utils.py`: безопасное чтение файлов, определение бинарных, нормализация путей.
- `tokenizer.py`: `count_tokens`, `chunk_text` — для соблюдения лимитов контекста.
- `changes.py`: `format_changes_md` — отчёт в `docs/changes.md`.

## Потоки данных

1. **Инициализация**: `main.py` → `scan_source` → `ScanResult`.
2. **Сравнение**: `CacheManager.diff_files` → категории изменений.
3. **Обработка**: `summarize_file` → `LLMClient.chat` → кэш или API.
4. **Вывод**: запись в `.ai-docs`, генерация `README.md` и `mkdocs.yml`.
5. **Сборка**: `mkdocs build` (внешний вызов, не входит в `ai_docs`).

## Особенности реализации

- **Параллелизм**: обработка файлов в `ThreadPoolExecutor`, количество потоков — `AI_DOCS_THREADS` или `--threads`.
- **Безопасность**: временные директории удаляются, бинарные файлы не читаются.
- **Гибкость**: `.ai-docs.yaml` позволяет переопределить расширения, шаблоны исключения.
- **Интеграция**: поддержка `.env`, совместимость с CI/CD (повторные запуски, кэширование).

Архитектура обеспечивает быструю, предсказуемую и масштабируемую генерацию документации с минимальным дублированием вычислений.
