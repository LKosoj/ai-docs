# Зависимости

Проект `ai_docs` зависит от внешних инструментов и Python-пакетов для корректной работы. Ниже приведены обязательные и опциональные зависимости с пояснением их назначения.

## Внешние системные зависимости

| Инструмент | Назначение | Обязательность |
|----------|-----------|---------------|
| `git` | Клонирование удалённых репозиториев при указании `--source` как URL | Обязательно при работе с Git |
| `mkdocs` | Сборка сайта документации (`mkdocs build`) | Обязательно при использовании `--mkdocs` |
| `mermaid-cli` (опционально) | Рендеринг Mermaid-диаграмм в статические изображения | Опционально, требуется при публикации |

## Python-зависимости (из `pyproject.toml`)

### Основные
- `requests` — HTTP-запросы к LLM API.
- `tiktoken` — токенизация текста для контроля длины контекста модели.
- `PyYAML` — парсинг конфигурационных файлов `.ai-docs.yaml` и `mkdocs.yml`.
- `python-dotenv` — загрузка переменных окружения из `.env`.
- `pathspec` — сопоставление путей с шаблонами `.gitignore` и `.build_ignore`.

### Для генерации документации (MkDocs)
- `mkdocs>=1.4` — фреймворк для сборки сайта документации.
- `mkdocs-mermaid2-plugin` — поддержка диаграмм Mermaid в Markdown.
- `pymdown-extensions` — расширенные Markdown-функции (например, `superfences` для кастомных блоков).

## Переменные окружения

| Переменная | Назначение | По умолчанию |
|-----------|-----------|-------------|
| `OPENAI_API_KEY` | Ключ для доступа к LLM API | Обязательна |
| `OPENAI_BASE_URL` | Базовый URL API (поддерживает OpenAI-совместимые эндпоинты) | `https://api.openai.com/v1` |
| `OPENAI_MODEL` | Модель для генерации (например, `gpt-4o-mini`) | `gpt-4o-mini` |
| `OPENAI_TEMPERATURE` | Уровень креативности генерации | `0.2` |
| `OPENAI_MAX_TOKENS` | Макс. токенов в ответе | `1200` |
| `OPENAI_CONTEXT_TOKENS` | Общий лимит контекста | `8192` |
| `AI_DOCS_THREADS` | Количество потоков обработки файлов | `4` |
| `AI_DOCS_LOCAL_SITE` | Режим локальной сборки (отключает `site_url` в `mkdocs.yml`) | `false` |

## Кэширование и производительность

- Кэш LLM-ответов хранится в `.ai_docs_cache/llm_cache.json` (ключ — SHA256 от payload).
- Индекс файлов и их хэшей — `.ai-docs/index.json`.
- Кэширование включено по умолчанию; отключается флагом `--no-cache`.

> **Примечание**: При использовании сторонних LLM-провайдеров (например, Ollama, LocalAI) укажите `OPENAI_BASE_URL` соответствующего локального эндпоинта.
