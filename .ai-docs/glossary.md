# Глоссарий

| Термин | Определение |
|-------|-----------|
| **LLM** | Large Language Model — языковая модель, используемая для генерации текста документации на основе анализа исходных файлов. В `ai_docs` поддерживается через OpenAI-совместимые API. |
| **Кэширование** | Механизм сохранения результатов LLM-запросов и хэшей файлов в `.ai_docs_cache/` для ускорения повторной генерации. Отключается флагом `--no-cache`. |
| **Домен** | Логическая область проекта, определяемая по характерным файлам: `Kubernetes`, `Terraform`, `Docker`, `CI/CD`, `Helm`. Используется для классификации и структурирования документации. |
| **Модуль** | Файл или директория с исходным кодом, подлежащая детальному описанию. Документация сохраняется в `.ai-docs/modules/`. |
| **Конфигурация** | Файл настройки (`.yaml`, `.json`, `Dockerfile`, `tf`, и др.). Описывается отдельно и попадает в `.ai-docs/configs/`. |
| **ScanResult** | Объект, возвращаемый сканером, содержащий список файлов с метаданными, корневой путь, источник и имя репозитория. |
| **`.ai-docs.yaml`** | Конфигурационный файл для кастомизации поведения сканера: расширения файлов, шаблоны исключения, домены. Создаётся автоматически при отсутствии. |
| **`.ai_docs_cache/`** | Директория с кэшем: `llm_cache.json` (ответы LLM) и `index.json` (хэши файлов). Используется `CacheManager` для инкрементальной обработки. |
| **`.build_ignore`** | Файл с пользовательскими шаблонами исключения, аналог `.gitignore`, но специфичный для `ai_docs`. |
| **`_index.json`** | Навигационный индекс, генерируемый в `.ai-docs/`. Определяет структуру и приоритеты модулей для навигации в документации. |
| **`FIXED_INCLUDE_PATTERNS`** | Встроенные шаблоны включения файлов независимо от расширений: `Dockerfile`, `Makefile`, `requirements.txt`, `go.mod`, `package.json`, `*.lock` и др. |
| **`DEFAULT_EXCLUDE_PATTERNS`** | Стандартные шаблоны исключения: `.git`, `__pycache__`, `.venv`, `node_modules`, `dist`, `build`, `ai_docs_site`, `.ai-docs`, `.ai_docs_cache`. |
| **`chunk_text`** | Утилита разбиения текста на части по токенам (с учётом `max_tokens` модели), чтобы уложиться в лимит контекста LLM. |
| **`CacheManager`** | Класс для управления кэшем файлов: сравнение хэшей, определение изменений (added/modified/deleted/unchanged), сохранение и загрузка состояния. |
| **`LLMClient`** | Клиент для отправки запросов к LLM с поддержкой кэширования, таймаутов, потокобезопасности и OpenAI-совместимых эндпоинтов. |
| **`summarize_file`** | Основная функция генерации описания файла. Использует LLM, промпты и нормализацию вывода. Поддерживает режимы: общий обзор, модуль, конфигурация. |
| **`build_mkdocs_yaml`** | Функция генерации конфигурации `mkdocs.yml` с поддержкой навигации, плагинов (mermaid2, search) и локального запуска. |
| **`file_map`** | Внутренняя структура данных, содержащая информацию о каждом файле: тип, домены, путь к резюме, хэш. Используется для управления генерацией. |
| **`changes.md`** | Автоматически обновляемый файл с отчётом о внесённых изменениях: добавленные/изменённые/удалённые файлы, перегенерированные разделы. |
| **`tiktoken`** | Библиотека для точного подсчёта токенов в тексте под модели OpenAI. Используется в `token_utils.py`. |
| **`--force`** | CLI-флаг, принудительно перезаписывающий существующий `README.md` без проверки. |
| **`--threads` / `AI_DOCS_THREADS`** | Параметр параллельной обработки файлов. Ускоряет генерацию, особенно в больших проектах. |
